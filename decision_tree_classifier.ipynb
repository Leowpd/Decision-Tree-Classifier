{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>&lt;100K</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>&gt;=10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50-59</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>&lt;100K</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>&gt;=10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30-39</td>\n",
       "      <td>Private</td>\n",
       "      <td>200K-300K</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8--9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50-59</td>\n",
       "      <td>Private</td>\n",
       "      <td>200K-300K</td>\n",
       "      <td>11th</td>\n",
       "      <td>&lt;8</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20-29</td>\n",
       "      <td>Private</td>\n",
       "      <td>&gt;300K</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>&gt;=10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>35-40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30156</th>\n",
       "      <td>20-29</td>\n",
       "      <td>Private</td>\n",
       "      <td>200K-300K</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>&gt;=10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30157</th>\n",
       "      <td>40-49</td>\n",
       "      <td>Private</td>\n",
       "      <td>100K-200K</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8--9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30158</th>\n",
       "      <td>50-59</td>\n",
       "      <td>Private</td>\n",
       "      <td>100K-200K</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8--9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30159</th>\n",
       "      <td>20-29</td>\n",
       "      <td>Private</td>\n",
       "      <td>200K-300K</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8--9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30160</th>\n",
       "      <td>50-59</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>200K-300K</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8--9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>35-40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30161 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age         workclass     fnlwgt   education education_num  \\\n",
       "0      30-39         State-gov      <100K   Bachelors          >=10   \n",
       "1      50-59  Self-emp-not-inc      <100K   Bachelors          >=10   \n",
       "2      30-39           Private  200K-300K     HS-grad          8--9   \n",
       "3      50-59           Private  200K-300K        11th            <8   \n",
       "4      20-29           Private      >300K   Bachelors          >=10   \n",
       "...      ...               ...        ...         ...           ...   \n",
       "30156  20-29           Private  200K-300K  Assoc-acdm          >=10   \n",
       "30157  40-49           Private  100K-200K     HS-grad          8--9   \n",
       "30158  50-59           Private  100K-200K     HS-grad          8--9   \n",
       "30159  20-29           Private  200K-300K     HS-grad          8--9   \n",
       "30160  50-59      Self-emp-inc  200K-300K     HS-grad          8--9   \n",
       "\n",
       "           marital_status         occupation   relationship   race     sex  \\\n",
       "0           Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1      Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2                Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3      Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4      Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "30156  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "30157  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "30158             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "30159       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "30160  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "      hours_per_week native_country  \n",
       "0              35-40  United-States  \n",
       "1                <35  United-States  \n",
       "2              35-40  United-States  \n",
       "3              35-40  United-States  \n",
       "4              35-40           Cuba  \n",
       "...              ...            ...  \n",
       "30156          35-40  United-States  \n",
       "30157          35-40  United-States  \n",
       "30158          35-40  United-States  \n",
       "30159            <35  United-States  \n",
       "30160          35-40  United-States  \n",
       "\n",
       "[30161 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "file_path = \"./data/adult_train_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data[[\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]]\n",
    "Y = data[[\"income_more50K\"]]\n",
    "\n",
    "feature_dict = {col: X[col].unique().tolist() for col in X.columns}\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our splitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, Y, feature, val):\n",
    "    mask_yes = X[feature] == val\n",
    "    mask_not = X[feature] != val\n",
    "    \n",
    "    X_yes = X[mask_yes]\n",
    "    Y_yes = Y[mask_yes]\n",
    "\n",
    "    X_not = X[mask_not]\n",
    "    Y_not = Y[mask_not]\n",
    "    \n",
    "    return (feature, val, X_yes, Y_yes, X_not, Y_not)\n",
    "\n",
    "\n",
    "def split_everything(X, Y):\n",
    "    splits = []\n",
    "    for feature in feature_dict:\n",
    "        for val in feature_dict[feature]:\n",
    "            splits.append(split(X, Y, feature, val))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our entropy and information gain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(Y):\n",
    "    total = Y.shape[0]\n",
    "    more50k = int(Y.sum().iloc[0])\n",
    "    less50k = total-more50k\n",
    "\n",
    "    if total != 0:\n",
    "        P_more50k = more50k / total\n",
    "        P_less50k = less50k / total\n",
    "    else:\n",
    "        P_more50k = 1\n",
    "        P_less50k = 1\n",
    "\n",
    "    if P_more50k == 0:\n",
    "        P_more50k = 1       # log(1) = 0 so this causes this probability to be ignored\n",
    "    if P_less50k == 0:\n",
    "        P_less50k = 1       # log(1) = 0 so this causes this probability to be ignored\n",
    "\n",
    "    return -P_more50k * math.log(P_more50k, 2) - P_less50k * math.log(P_less50k, 2)\n",
    "\n",
    "\n",
    "def infogain(Y_entropy, n, Y_yes, Y_no):\n",
    "    n_yes = Y_yes.shape[0]\n",
    "    n_no = Y_no.shape[0]\n",
    "\n",
    "    return Y_entropy - (n_yes / n)*entropy(Y_yes) - (n_no / n)*entropy(Y_no)\n",
    "\n",
    "\n",
    "def all_infogains(Y, splits):\n",
    "    all_the_infogains = []\n",
    "\n",
    "    Y_entropy = entropy(Y)\n",
    "    for split in splits:\n",
    "        this_infogain = infogain(Y_entropy, Y.shape[0], split[3], split[5])\n",
    "        info_tuple = (split[0], split[1], this_infogain)\n",
    "        all_the_infogains.append(info_tuple)\n",
    "\n",
    "    return all_the_infogains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a node class that we will use to construct our decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, split, feature, left, right, leaf: bool, label=None):\n",
    "        self.split = split\n",
    "        self.feature = feature\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.leaf = leaf\n",
    "        self.label = label  # only for leaves\n",
    "\n",
    "\n",
    "    def get_split(self):\n",
    "        return self.split\n",
    "    \n",
    "\n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "    \n",
    "\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    \n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "\n",
    "    def get_leaf(self):\n",
    "        return self.leaf\n",
    "    \n",
    "\n",
    "    def get_label(self):\n",
    "        return self.label\n",
    "    \n",
    "\n",
    "    def print_tree(self, prefix=\"\", left=True, max_length=None, line_count=[0]):\n",
    "        if max_length is not None and line_count[0] >= max_length:\n",
    "            return\n",
    "        \n",
    "        connector = \"â”œâ”€â”€ \" if left else \"â””â”€â”€ \"\n",
    "        if self.leaf:\n",
    "            print(prefix + connector + f\"Leaf: {self.label}\")\n",
    "        else:\n",
    "            print(prefix + connector + f\"{self.feature}: {self.split}\")\n",
    "        \n",
    "        line_count[0] += 1\n",
    "        \n",
    "        if self.left and (max_length is None or line_count[0] < max_length):\n",
    "            self.left.print_tree(prefix + (\"â”‚   \" if left else \"    \"), True, max_length, line_count)\n",
    "        if self.right and (max_length is None or line_count[0] < max_length):\n",
    "            self.right.print_tree(prefix + (\"â”‚   \" if left else \"    \"), False, max_length, line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a recursive function that trains our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y):\n",
    "\n",
    "    if Y['income_more50K'].nunique() == 1:\n",
    "        return Node(None, None, None, None, True, round((int(Y.sum().iloc[0]))/Y.shape[0]))\n",
    "\n",
    "    splits = split_everything(X, Y)\n",
    "    infogains = all_infogains(Y, splits)\n",
    "\n",
    "    infogains_frame = pd.DataFrame(infogains, columns =['Feature', 'Value', 'Information Gain'])\n",
    "    #infogains_frame.sort_values('Information Gain', ascending=False, inplace=True)\n",
    "    biggest_infogain = infogains_frame.nlargest(1, 'Information Gain')\n",
    "    split_by_value = biggest_infogain['Value'].iloc[0]\n",
    "    split_data = [data_tuple for data_tuple in splits if data_tuple[1] == split_by_value][0]\n",
    "\n",
    "    X_yes = split_data[2]\n",
    "    Y_yes = split_data[3]\n",
    "    X_not = split_data[4]\n",
    "    Y_not = split_data[5]\n",
    "\n",
    "    if Y_yes.shape[0] == 0 or Y_not.shape[0] == 0:\n",
    "        return Node(None, None, None, None, True, round((int(Y.sum().iloc[0]))/Y.shape[0]))\n",
    "\n",
    "    return Node(split_by_value, split_data[0], train(X_yes, Y_yes), train(X_not, Y_not), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”œâ”€â”€ marital_status: Married-civ-spouse\n",
      "â”‚   â”œâ”€â”€ education_num: >=10\n",
      "â”‚   â”‚   â”œâ”€â”€ education: Some-college\n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ age: 20-29\n",
      "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hours_per_week: <35\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fnlwgt: 200K-300K\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ occupation: Adm-clerical\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ relationship: Husband\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ occupation: Other-service\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ fnlwgt: 200K-300K\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ workclass: Private\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ relationship: Husband\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚       â””â”€â”€ occupation: Machine-op-inspct\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”œâ”€â”€ fnlwgt: 200K-300K\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ relationship: Husband\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â”‚   â”œâ”€â”€ hours_per_week: 35-40\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â”‚   â””â”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚           â””â”€â”€ workclass: State-gov\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚               â”œâ”€â”€ occupation: Exec-managerial\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚               â”‚   â”œâ”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚               â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚               â””â”€â”€ occupation: Adm-clerical\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”œâ”€â”€ race: Amer-Indian-Eskimo\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚   â””â”€â”€ fnlwgt: <100K\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚       â”œâ”€â”€ workclass: Private\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚       â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚       â”‚   â””â”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚       â””â”€â”€ workclass: Local-gov\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚           â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚           â””â”€â”€ relationship: Own-child\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚               â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚               â””â”€â”€ relationship: Other-relative\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                   â””â”€â”€ race: Asian-Pac-Islander\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                       â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                       â””â”€â”€ hours_per_week: 35-40\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                           â”œâ”€â”€ relationship: Husband\n",
      "â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚                           â”‚   â”œâ”€â”€ race: Black\n"
     ]
    }
   ],
   "source": [
    "decision_tree_depth_full = train(X,Y)\n",
    "\n",
    "decision_tree_depth_full.print_tree(max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "The full depth tree above is 11289 lines long, so it is presented truncated to the top 50 lines ease of viewing. The left branch (or \"yes\" branch) of each rule split is printed first with the right branch (or \"no\" branch) printed after.\n",
    "\n",
    "Given the depth of this tree we would expect it to have a very high accuracy when classifying the training data, but it should be noted that as we only have 12 features per example (where each feature can have 2 - 40 different values) and 30161 examples, we will have a large number of identical examples (and indeed, exploring the raw csv confirms this). If any of these identical examples have different labels, these will be impossible for this model to tell apart. Hence, we would not expect 100% accuracy when classifying our training data but we would still expect out accuracy to be as high as is possible given the limitations of the data.\n",
    "\n",
    "But of course, thanks to the fundamental trade off, we would expect the variance of our model to be very high, and hence this model will likely preform quite poorly on our test set thanks to overfitting the training set. A solution to this is to control the depth of our tree, which we explore in Part B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_depth(X, Y, max_depth, current_depth=0):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        return Node(None, None, None, None, True, round((int(Y.sum().iloc[0]))/Y.shape[0]))\n",
    "\n",
    "    if Y['income_more50K'].nunique() == 1:\n",
    "        return Node(None, None, None, None, True, round((int(Y.sum().iloc[0]))/Y.shape[0]))\n",
    "\n",
    "    splits = split_everything(X, Y)\n",
    "    infogains = all_infogains(Y, splits)\n",
    "\n",
    "    infogains_frame = pd.DataFrame(infogains, columns =['Feature', 'Value', 'Information Gain'])\n",
    "    #infogains_frame.sort_values('Information Gain', ascending=False, inplace=True)\n",
    "    biggest_infogain = infogains_frame.nlargest(1, 'Information Gain')\n",
    "    split_by_value = biggest_infogain['Value'].iloc[0]\n",
    "    split_data = [data_tuple for data_tuple in splits if data_tuple[1] == split_by_value][0]\n",
    "\n",
    "    X_yes = split_data[2]\n",
    "    Y_yes = split_data[3]\n",
    "    X_not = split_data[4]\n",
    "    Y_not = split_data[5]\n",
    "\n",
    "    if Y_yes.shape[0] == 0 or Y_not.shape[0] == 0:\n",
    "        return Node(None, None, None, None, True, round((int(Y.sum().iloc[0]))/Y.shape[0]))\n",
    "\n",
    "    return Node(split_by_value, split_data[0], train_with_depth(X_yes, Y_yes, max_depth, current_depth+1), train_with_depth(X_not, Y_not, max_depth, current_depth+1), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”œâ”€â”€ marital_status: Married-civ-spouse\n",
      "â”‚   â”œâ”€â”€ education_num: >=10\n",
      "â”‚   â”‚   â”œâ”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â””â”€â”€ hours_per_week: >40\n",
      "â”‚       â”œâ”€â”€ Leaf: 0\n",
      "â”‚       â””â”€â”€ Leaf: 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_depth_2 = train_with_depth(X, Y, 2)\n",
    "\n",
    "decision_tree_depth_2.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”œâ”€â”€ marital_status: Married-civ-spouse\n",
      "â”‚   â”œâ”€â”€ education_num: >=10\n",
      "â”‚   â”‚   â”œâ”€â”€ education: Some-college\n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â””â”€â”€ education_num: 8--9\n",
      "â”‚   â”‚       â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚       â””â”€â”€ Leaf: 0\n",
      "â”‚   â””â”€â”€ hours_per_week: >40\n",
      "â”‚       â”œâ”€â”€ education_num: >=10\n",
      "â”‚       â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚       â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚       â””â”€â”€ occupation: Prof-specialty\n",
      "â”‚           â”œâ”€â”€ Leaf: 0\n",
      "â”‚           â””â”€â”€ Leaf: 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_depth_3 = train_with_depth(X, Y, 3)\n",
    "\n",
    "decision_tree_depth_3.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”œâ”€â”€ marital_status: Married-civ-spouse\n",
      "â”‚   â”œâ”€â”€ education_num: >=10\n",
      "â”‚   â”‚   â”œâ”€â”€ education: Some-college\n",
      "â”‚   â”‚   â”‚   â”œâ”€â”€ age: 20-29\n",
      "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚   â””â”€â”€ education: Assoc-voc\n",
      "â”‚   â”‚   â”‚       â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚   â”‚       â””â”€â”€ Leaf: 1\n",
      "â”‚   â”‚   â””â”€â”€ education_num: 8--9\n",
      "â”‚   â”‚       â”œâ”€â”€ age: 20-29\n",
      "â”‚   â”‚       â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚       â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚   â”‚       â””â”€â”€ hours_per_week: >40\n",
      "â”‚   â”‚           â”œâ”€â”€ Leaf: 0\n",
      "â”‚   â”‚           â””â”€â”€ Leaf: 0\n",
      "â”‚   â””â”€â”€ hours_per_week: >40\n",
      "â”‚       â”œâ”€â”€ education_num: >=10\n",
      "â”‚       â”‚   â”œâ”€â”€ age: 20-29\n",
      "â”‚       â”‚   â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚       â”‚   â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚       â”‚   â””â”€â”€ age: 20-29\n",
      "â”‚       â”‚       â”œâ”€â”€ Leaf: 0\n",
      "â”‚       â”‚       â””â”€â”€ Leaf: 0\n",
      "â”‚       â””â”€â”€ occupation: Prof-specialty\n",
      "â”‚           â”œâ”€â”€ age: 20-29\n",
      "â”‚           â”‚   â”œâ”€â”€ Leaf: 0\n",
      "â”‚           â”‚   â””â”€â”€ Leaf: 0\n",
      "â”‚           â””â”€â”€ occupation: Exec-managerial\n",
      "â”‚               â”œâ”€â”€ Leaf: 0\n",
      "â”‚               â””â”€â”€ Leaf: 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_depth_4 = train_with_depth(X, Y, 4)\n",
    "\n",
    "decision_tree_depth_4.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Controlling the tree depth produces much smaller trees that are far faster to build. Continuing our discussion of the fundamental trade off from our previous discussion, we would expect our smaller trees to be better for generalisation to more data. These models are much less fitted to our training data specifically which we would give these models lower variance. Of course, for our especially shallow depths (e.g. our tree with depth 2), this depth control may not provide enough splits for good classification of data. We will next use classifier evaluation metrics to evaluate which models are the most effective. In future we may want to set depth as a hyper-parameter and experiment for a more optimum depth.\n",
    "\n",
    "Something worth noting is that we have some branches that lead to two leaves with the same label. E.g. in our depth 4 tree, our bottom most split \"occupation: Exec-managerial\" has two leaves with label 0 (income not more than 50k). This creates a less-than-minimal tree where we could just equivalently turn the \"occupation: Exec-managerial\" node into a leaf with label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining the functions that run new data through our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(current_node, example):\n",
    "    if current_node.get_leaf():\n",
    "        return current_node.get_label()\n",
    "    \n",
    "    split = current_node.get_split()\n",
    "    if split in example:\n",
    "        return predict_example(current_node.get_left(), example)\n",
    "    return predict_example(current_node.get_right(), example)\n",
    "\n",
    "\n",
    "def predict_all(X, decision_tree_root):\n",
    "    Y_list = []\n",
    "    for example in X.itertuples():\n",
    "        Y_list.append(predict_example(decision_tree_root, list(example)))\n",
    "\n",
    "    return pd.DataFrame({\"income_more50K\": Y_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for calculating our classifier evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(Y_predicted: list, Y_true: list):\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(Y_true)):\n",
    "        if Y_true[i] == 0 and Y_predicted[i] == 0:\n",
    "            TN += 1\n",
    "        elif Y_true[i] == 1 and Y_predicted[i] == 1:\n",
    "            TP += 1\n",
    "        elif Y_true[i] == 0 and Y_predicted[i] == 1:\n",
    "            FP += 1\n",
    "        elif Y_true[i] == 1 and Y_predicted[i] == 0:\n",
    "            FN += 1\n",
    "    \n",
    "    return (TN, TP, FP, FN)\n",
    "\n",
    "\n",
    "def accuracy(confusion_tuple):\n",
    "    TN = confusion_tuple[0]\n",
    "    TP = confusion_tuple[1]\n",
    "    FP = confusion_tuple[2]\n",
    "    FN = confusion_tuple[3]\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "\n",
    "def precision(confusion_tuple):\n",
    "    TN = confusion_tuple[0]\n",
    "    TP = confusion_tuple[1]\n",
    "    FP = confusion_tuple[2]\n",
    "    FN = confusion_tuple[3]\n",
    "    return (TP) / (TP + FP)\n",
    "\n",
    "\n",
    "def recall(confusion_tuple):\n",
    "    TN = confusion_tuple[0]\n",
    "    TP = confusion_tuple[1]\n",
    "    FP = confusion_tuple[2]\n",
    "    FN = confusion_tuple[3]\n",
    "    return (TP) / (TP + FN)\n",
    "\n",
    "\n",
    "def f_measure(precision, recall, beta=1):\n",
    "    return (1 + beta) * (precision*recall) / (beta**2 * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving our test data and obtaining our evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_test = \"./data/adult_test_data.csv\"\n",
    "data_test = pd.read_csv(file_path_test)\n",
    "\n",
    "X_test = data_test[[\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]]\n",
    "Y_test = data_test[[\"income_more50K\"]]\n",
    "\n",
    "Y_pred_depth_full = predict_all(X_test, decision_tree_depth_full)\n",
    "Y_pred_depth_2 = predict_all(X_test, decision_tree_depth_2)\n",
    "Y_pred_depth_3 = predict_all(X_test, decision_tree_depth_3)\n",
    "Y_pred_depth_4 = predict_all(X_test, decision_tree_depth_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth | Accuracy    Precision   Recall      F1 Measure  \n",
      "------+-------------------------------------------------\n",
      "Full  | 0.7943      0.5894      0.5365      0.5617      \n",
      "2     | 0.8066      0.6009      0.6341      0.6170      \n",
      "3     | 0.8146      0.6735      0.4762      0.5579      \n",
      "4     | 0.8170      0.7049      0.4389      0.5410      \n"
     ]
    }
   ],
   "source": [
    "confusion_depth_full = confusion(list(Y_pred_depth_full[\"income_more50K\"]), list(Y_test[\"income_more50K\"]))\n",
    "confusion_depth_2 = confusion(list(Y_pred_depth_2[\"income_more50K\"]), list(Y_test[\"income_more50K\"]))\n",
    "confusion_depth_3 = confusion(list(Y_pred_depth_3[\"income_more50K\"]), list(Y_test[\"income_more50K\"]))\n",
    "confusion_depth_4 = confusion(list(Y_pred_depth_4[\"income_more50K\"]), list(Y_test[\"income_more50K\"]))\n",
    "\n",
    "depth_full_accuracy = accuracy(confusion_depth_full)\n",
    "depth_full_precision = precision(confusion_depth_full)\n",
    "depth_full_recall = recall(confusion_depth_full)\n",
    "depth_full_f1_measure = f_measure(depth_full_precision, depth_full_recall)\n",
    "\n",
    "depth_2_accuracy = accuracy(confusion_depth_2)\n",
    "depth_2_precision = precision(confusion_depth_2)\n",
    "depth_2_recall = recall(confusion_depth_2)\n",
    "depth_2_f1_measure = f_measure(depth_2_precision, depth_2_recall)\n",
    "\n",
    "depth_3_accuracy = accuracy(confusion_depth_3)\n",
    "depth_3_precision = precision(confusion_depth_3)\n",
    "depth_3_recall = recall(confusion_depth_3)\n",
    "depth_3_f1_measure = f_measure(depth_3_precision, depth_3_recall)\n",
    "\n",
    "depth_4_accuracy = accuracy(confusion_depth_4)\n",
    "depth_4_precision = precision(confusion_depth_4)\n",
    "depth_4_recall = recall(confusion_depth_4)\n",
    "depth_4_f1_measure = f_measure(depth_4_precision, depth_4_recall)\n",
    "\n",
    "\n",
    "print(f\"{'Depth':<6}| {'Accuracy':<12}{'Precision':<12}{'Recall':<12}{'F1 Measure':<12}\\n{'-' * 6}+{'-' * 49}\")\n",
    "print(f\"{'Full':<6}| {depth_full_accuracy:<12.4f}{depth_full_precision:<12.4f}{depth_full_recall:<12.4f}{depth_full_f1_measure:<12.4f}\")\n",
    "print(f\"{'2':<6}| {depth_2_accuracy:<12.4f}{depth_2_precision:<12.4f}{depth_2_recall:<12.4f}{depth_2_f1_measure:<12.4f}\")\n",
    "print(f\"{'3':<6}| {depth_3_accuracy:<12.4f}{depth_3_precision:<12.4f}{depth_3_recall:<12.4f}{depth_3_f1_measure:<12.4f}\")\n",
    "print(f\"{'4':<6}| {depth_4_accuracy:<12.4f}{depth_4_precision:<12.4f}{depth_4_recall:<12.4f}{depth_4_f1_measure:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "We can see that for Accuracy our full depth tree is the worst performing model (at 79.4%), followed by our depth 2 tree, then depth 3, with our depth 4 tree performing the best (at 81.7%). This order is the same for our Precision metric, this time with 58.9% as our lowest and 70.5% as our highest. For Recall, we have our depth 4 tree at 43.9%, followed by depth 3, then full depth, giving us our depth 2 tree as our best performing model at 63.4%. For our F1 Measure we have the same order, with 54.1% as our lowest score and 61.7% as our highest.\n",
    "\n",
    "Our Accuracy scores suggest that our model initially gets more accurate as depth increases, but there is a point where this switches, and exploration of depth as a hyper-parameter would be needed to give a better idea of where this point is. The Accuracy scores are also the highest of all our metrics, indicating that all our models are comparatively good at general classifying. We will further discuss this below. Our Precision scores show a similar story, indicating both that tentitive increases in depth may continue to result in better performance (but again we cannot be sure without further experimentation) and that when our models (particularly depths 3 & 4) classify positive labels they are indeed typically positive (i.e. low false positive rate). It follows then that all our models perform poorly at Recall, showing that we have a fairly high false negative rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the information gain (infogain) criterion to decide on splits for our trees. This criterion bases splits purely on the infogain calculation, giving no additional preference to positive or negative labels. This makes this a good criterion to use when all we care is about classifying our data, with no preference for correctly classifying positives or negatives. The other splitting criterion we explored in lectures was \"decision stump accuracy\" or \"classification accuracy\" which decides on splits based on \"if we use this rule, how many examples do we label correctly?\". Like infogain, this is a criterion that you would use if you only care about correctly classifying the data. This is typically a poorer method of choosing splits as it gives no insight to how useful a split will be for generating more splits. Hence, we would expect the \"classification accuracy\" criterion to result in poorer performance from models using it.\n",
    "\n",
    "If we had more context for what we were using the data for, another splitting criterion may be better. E.g. if we were calcuating the expected income from tax for everyone over the 50k income threshold, we may care more about reducing false positives as we may prefer to end up with more money than expected rather than less money. In this case we may wish to have some splitting criterion that weighs the information gained from classifying positive values higher, which would result in a higher Precision score for the resulting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously observed, our Accuracy increases as our model gets deeper for small depths, but is worse for our full depth tree. This is a strong indicator that our full depth tree is very overfitted to the training data. When we look at Recall, we see that our best performing model is our depth 2 tree, and that our score gets worse in our depth 3 and 4 trees. This shows that our model quickly becomes worse at identifying positive labels at increases depth, which may indicate that our models are starting to overfit to the training data even in these early depths. Given that the difference between the best and the worst Accuracy scores is only ~2.3% while the difference between the best and the worst Recall scores is ~19.5%, this indicates that our model starts overfitting quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "361ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
